{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh0487/Harsh_INFO5731_fall2024/blob/main/harsh_patel_exercise_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# #Research Question: How does the sentiment of social media posts about electric vehicles (EVs) affect consumer purchasing decisions?\n",
        "\n",
        "# #Background: As the world shifts towards sustainable energy sources, electric vehicles are gaining popularity. Understanding the sentiment of social media posts about EVs can provide valuable insights for manufacturers, policymakers, and marketers. This research question aims to investigate the relationship between social media sentiment and consumer purchasing decisions.\n",
        "\n",
        "# #Data Requirements:\n",
        "\n",
        "#         Social media posts: Collect 1000 social media posts (tweets, Facebook posts, Instagram comments, etc.) about electric vehicles from various platforms.\n",
        "#         Sentiment analysis: Label each post as positive, negative, or neutral based on its sentiment towards EVs.\n",
        "#         Purchasing decisions: Collect data on whether the user who posted about EVs has purchased an EV or not.\n",
        "\n",
        "\n",
        "# #Data Collection Steps:\n",
        "\n",
        "#         Social media scraping: Use APIs or web scraping tools (e.g., Twitter API, Facebook Graph API, Scrapy) to collect social media posts about electric vehicles.\n",
        "#         Sentiment analysis: Use natural language processing (NLP) techniques or machine learning models (e.g., Naive Bayes, Support Vector Machines) to label each post as positive, negative, or neutral.\n",
        "# Survey or questionnaire: Design a survey or questionnaire to collect information about the user's purchasing decisions (e.g., have they purchased an EV, are they planning to purchase one, etc.).\n",
        "#         Data storage: Store the collected data in a structured format (e.g., CSV, JSON) for analysis."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = 'oOW12dmMq'\n",
        "consumer_secret = 'e6Jqg21Ds0ONt6'\n",
        "access_token = '18344530259836928'\n",
        "access_token_secret = ''\n",
        "\n",
        "# Set up Twitter API connection\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define search query\n",
        "query = '#electricvehicles OR #EVs OR \"electric cars\"'\n",
        "\n",
        "# Collect tweets\n",
        "#tweets = tweepy.Cursor(api.search, q=query, lang='en').items(1000)\n",
        "tweets = tweepy.Cursor(api.search_tweets, q=query, lang='en').items(1000)\n",
        "\n",
        "# Create a pandas dataframe to store the data\n",
        "df = pd.DataFrame(columns=['text', 'sentiment', 'purchasing_decision'])\n",
        "\n",
        "# Loop through tweets and perform sentiment analysis\n",
        "for tweet in tweets:\n",
        "    text = tweet.text\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        sentiment_label = 'positive'\n",
        "    elif sentiment < 0:\n",
        "        sentiment_label = 'negative'\n",
        "    else:\n",
        "        sentiment_label = 'neutral'\n",
        "\n",
        "    # Assume we have a way to collect purchasing decision data (e.g., survey)\n",
        "    purchasing_decision = 'yes'  # or 'no'\n",
        "\n",
        "    # Add data to the dataframe\n",
        "    df = df.append({'text': text, 'sentiment': sentiment_label, 'purchasing_decision': purchasing_decision}, ignore_index=True)\n",
        "\n",
        "# Save the dataframe to a CSV file\n",
        "df.to_csv('ev_tweets.csv', index=False)"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b83020f-3733-4817-b899-3e48347c306e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 0 articles.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "\n",
        "def fetch_semantic_scholar_articles(keyword, num_articles):\n",
        "    articles = []\n",
        "    base_url = \"https://www.semanticscholar.org/search?q={}&year=2014,2024&sort=relevance\"\n",
        "\n",
        "    for start in range(0, num_articles, 10):\n",
        "        url = base_url.format(keyword)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        results = soup.find_all('div', class_='search-result')\n",
        "        for result in results:\n",
        "            title = result.find('a', class_='title').text.strip()\n",
        "            venue = result.find('span', class_='venue').text.strip() if result.find('span', class_='venue') else \"N/A\"\n",
        "            year = result.find('span', class_='year').text.strip() if result.find('span', class_='year') else \"N/A\"\n",
        "            authors = ', '.join([author.text for author in result.find_all('a', class_='author')])\n",
        "            abstract = result.find('div', class_='abstract').text.strip() if result.find('div', class_='abstract') else \"No abstract available\"\n",
        "\n",
        "            articles.append({\n",
        "                \"Title\": title,\n",
        "                \"Venue\": venue,\n",
        "                \"Year\": year,\n",
        "                \"Authors\": authors,\n",
        "                \"Abstract\": abstract\n",
        "            })\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        if len(articles) >= num_articles:\n",
        "            break\n",
        "\n",
        "    return articles[:num_articles]\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    keyword = \"XYZ\"\n",
        "    num_articles = 1000\n",
        "\n",
        "    all_articles = []\n",
        "\n",
        "\n",
        "    all_articles.extend(fetch_semantic_scholar_articles(keyword, num_articles))\n",
        "\n",
        "    all_articles = all_articles[:num_articles]\n",
        "\n",
        "    with open('articles.json', 'w') as f:\n",
        "        json.dump(all_articles, f, indent=4)\n",
        "\n",
        "    print(f\"Collected {len(all_articles)} articles.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# #Introduction to Octoparse\n",
        "# #Octoparse is a web scraping tool that provides an intuitive interface for extracting data from websites. It allows users to create scraping tasks without needing to write code, making it accessible for those who may not have programming experience.\n",
        "\n",
        "# #Steps for Web Scraping with Octoparse\n",
        "\n",
        "#     1)Download and Install Octoparse:-\n",
        "#           Go to the Octoparse website and download the application.\n",
        "#           Install the software on your computer.\n",
        "\n",
        "#     2)Create a New Task:-\n",
        "#           Open Octoparse and click on \"New Task\" to start a new scraping project.\n",
        "\n",
        "#     3)Enter the Website URL:-\n",
        "#           Enter the URL of the website you want to scrape into the \"Enter URL\" field and click \"Start.\"\n",
        "\n",
        "#     4)Set Up the Scraping Task:-\n",
        "#           Navigate the Website: Use the built-in browser to navigate through the website and locate the data you want to extract.\n",
        "#           Select Data: Click on the elements you want to scrape. Octoparse will highlight the selected data and suggest similar items to include.\n",
        "#           Define Data Fields: Assign meaningful names to the data fields you are extracting (e.g., title, price, date).\n",
        "\n",
        "#     5)Customize Data Extraction:-\n",
        "#           Pagination: If the data spans multiple pages, set up pagination to navigate through all pages.\n",
        "#           Filters: Apply any necessary filters to refine your data extraction.\n",
        "\n",
        "#     6)Run the Task :-\n",
        "#           Click on \"Save & Run\" to start the extraction process. You can choose to run the task on your local machine or on the cloud.\n",
        "\n",
        "#     7)Export Data :-\n",
        "#           Once the scraping is complete, go to the \"Data\" tab.\n",
        "#           Choose the \"Export\" option and select your preferred format (e.g., CSV, Excel).\n",
        "#           Download the file to your computer.\n",
        "\n",
        "\n",
        "# #Final Output\n",
        "# #The extracted data will be available in the format you selected. For instance, if you export the data as a CSV file, it will be organized into rows and columns that can be opened with spreadsheet software like Excel.\n",
        "\n",
        "# #Uploading and Sharing the Document\n",
        "#     After exporting the data, you will need to upload the document to a shared storage service like UNT OneDrive or Google Drive and provide a publicly accessible link.\n",
        "#     Since I cannot directly upload files or generate real-time links, hereâ€™s an example of how you would share the link in your document:\n",
        "\n",
        "# #Document Upload Instructions:\n",
        "\n",
        "#     Upload your CSV or Excel file to UNT OneDrive or another file-sharing service.\n",
        "#     Set the file sharing settings to \"Public\" or \"Anyone with the link.\"\n",
        "#     Copy the sharing link.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "It is getting really though and its challenging.In my question_2, i cant put credentails but my whole programme is right."
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}