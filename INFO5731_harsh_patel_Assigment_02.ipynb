{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh0487/Harsh_INFO5731_fall2024/blob/main/INFO5731_harsh_patel_Assigment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5a36b5-b887-473e-a659-3bce490bbceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Scraping reviews for movie ID: tt14826022\n",
            "Scraping reviews for movie ID: tt1630029\n",
            "Saved 1000 reviews to imdb_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_imdb_reviews(movie_id, max_reviews=1000):\n",
        "    reviews = []\n",
        "    page_key = None\n",
        "    base_url = f'https://www.imdb.com/title/{movie_id}/reviews/_ajax'\n",
        "\n",
        "    while len(reviews) < max_reviews:\n",
        "        url = base_url if page_key is None else f'{base_url}?paginationKey={page_key}'\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        review_containers = soup.find_all('div', class_='review-container')\n",
        "\n",
        "        if not review_containers:\n",
        "            print(\"No more reviews found.\")\n",
        "            break\n",
        "\n",
        "        for container in review_containers:\n",
        "            if len(reviews) >= max_reviews:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                review_title = container.find('a', class_='title').get_text(strip=True)\n",
        "                review_content = container.find('div', class_='text').get_text(strip=True)\n",
        "                review_rating = container.find('span', class_='rating-other-user-rating')\n",
        "                review_rating = review_rating.get_text(strip=True) if review_rating else None\n",
        "                review_date = container.find('span', class_='review-date').get_text(strip=True)\n",
        "                review_username = container.find('span', class_='display-name-link').get_text(strip=True)\n",
        "\n",
        "                reviews.append({\n",
        "                    'Title': review_title,\n",
        "                    'Content': review_content,\n",
        "                    'Rating': review_rating,\n",
        "                    'Date': review_date,\n",
        "                    'Username': review_username\n",
        "                })\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "        pagination_key_tag = soup.find('div', class_='load-more-data')\n",
        "        page_key = pagination_key_tag.get('data-key') if pagination_key_tag else None\n",
        "\n",
        "        if not page_key:\n",
        "            break\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return reviews\n",
        "\n",
        "def save_reviews_to_csv(movie_ids, filename=\"imdb_reviews.csv\"):\n",
        "    all_reviews = []\n",
        "\n",
        "    for movie_id in movie_ids:\n",
        "        print(f\"Scraping reviews for movie ID: {movie_id}\")\n",
        "        reviews = get_imdb_reviews(movie_id)\n",
        "        all_reviews.extend(reviews)\n",
        "\n",
        "        if len(all_reviews) >= 1000:\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(all_reviews[:1000])\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {len(df)} reviews to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    movie_ids = ['tt14826022', 'tt1630029']\n",
        "    save_reviews_to_csv(movie_ids)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3b122f-7d3c-49cf-f96d-f8b6c76718a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "                                               Title  \\\n",
            "0            The racism actually got on my nerves...   \n",
            "1                                      Those People.   \n",
            "2                             Astonishingly annoying   \n",
            "3                   You People made a terrible movie   \n",
            "4  The most annoying film of 2023, it made me so ...   \n",
            "\n",
            "                                             Content  \n",
            "0  The black side actually aggravated me more tha...  \n",
            "1  You know, the ones who wrote all the cliched n...  \n",
            "2  It's been some time that I've been so bewilder...  \n",
            "3  How did this ever get made? Old Man Jonah Hill...  \n",
            "4  When mum turned to watch this godforsaken movi...  \n",
            "\n",
            "After Removing Special Characters:\n",
            "                                             Content  \\\n",
            "0  The black side actually aggravated me more tha...   \n",
            "1  You know, the ones who wrote all the cliched n...   \n",
            "2  It's been some time that I've been so bewilder...   \n",
            "3  How did this ever get made? Old Man Jonah Hill...   \n",
            "4  When mum turned to watch this godforsaken movi...   \n",
            "\n",
            "                                     Cleaned_Content  \n",
            "0  The black side actually aggravated me more tha...  \n",
            "1  You know the ones who wrote all the cliched no...  \n",
            "2  Its been some time that Ive been so bewildered...  \n",
            "3  How did this ever get made Old Man Jonah Hill ...  \n",
            "4  When mum turned to watch this godforsaken movi...  \n",
            "\n",
            "After Removing Numbers:\n",
            "                                             Content  \\\n",
            "0  The black side actually aggravated me more tha...   \n",
            "1  You know, the ones who wrote all the cliched n...   \n",
            "2  It's been some time that I've been so bewilder...   \n",
            "3  How did this ever get made? Old Man Jonah Hill...   \n",
            "4  When mum turned to watch this godforsaken movi...   \n",
            "\n",
            "                                     Cleaned_Content  \n",
            "0  The black side actually aggravated me more tha...  \n",
            "1  You know the ones who wrote all the cliched no...  \n",
            "2  Its been some time that Ive been so bewildered...  \n",
            "3  How did this ever get made Old Man Jonah Hill ...  \n",
            "4  When mum turned to watch this godforsaken movi...  \n",
            "\n",
            "After Removing Stopwords:\n",
            "                                             Content  \\\n",
            "0  The black side actually aggravated me more tha...   \n",
            "1  You know, the ones who wrote all the cliched n...   \n",
            "2  It's been some time that I've been so bewilder...   \n",
            "3  How did this ever get made? Old Man Jonah Hill...   \n",
            "4  When mum turned to watch this godforsaken movi...   \n",
            "\n",
            "                                     Cleaned_Content  \n",
            "0  black side actually aggravated anything Im bla...  \n",
            "1  know ones wrote cliched nonsense shallow dialo...  \n",
            "2  time Ive bewildered frustrated watching filmTh...  \n",
            "3  ever get made Old Man Jonah Hill miscast year ...  \n",
            "4  mum turned watch godforsaken movie Netflix hon...  \n",
            "\n",
            "After Lowercasing Texts:\n",
            "                                             Content  \\\n",
            "0  The black side actually aggravated me more tha...   \n",
            "1  You know, the ones who wrote all the cliched n...   \n",
            "2  It's been some time that I've been so bewilder...   \n",
            "3  How did this ever get made? Old Man Jonah Hill...   \n",
            "4  When mum turned to watch this godforsaken movi...   \n",
            "\n",
            "                                     Cleaned_Content  \n",
            "0  black side actually aggravated anything im bla...  \n",
            "1  know ones wrote cliched nonsense shallow dialo...  \n",
            "2  time ive bewildered frustrated watching filmth...  \n",
            "3  ever get made old man jonah hill miscast year ...  \n",
            "4  mum turned watch godforsaken movie netflix hon...  \n",
            "\n",
            "After Stemming:\n",
            "                                     Cleaned_Content  \\\n",
            "0  black side actually aggravated anything im bla...   \n",
            "1  know ones wrote cliched nonsense shallow dialo...   \n",
            "2  time ive bewildered frustrated watching filmth...   \n",
            "3  ever get made old man jonah hill miscast year ...   \n",
            "4  mum turned watch godforsaken movie netflix hon...   \n",
            "\n",
            "                                     Stemmed_Content  \n",
            "0  black side actual aggrav anyth im black almost...  \n",
            "1  know one wrote clich nonsens shallow dialogu a...  \n",
            "2  time ive bewild frustrat watch filmthi talent ...  \n",
            "3  ever get made old man jonah hill miscast year ...  \n",
            "4  mum turn watch godforsaken movi netflix honest...  \n",
            "\n",
            "After Lemmatization:\n",
            "                                     Stemmed_Content  \\\n",
            "0  black side actual aggrav anyth im black almost...   \n",
            "1  know one wrote clich nonsens shallow dialogu a...   \n",
            "2  time ive bewild frustrat watch filmthi talent ...   \n",
            "3  ever get made old man jonah hill miscast year ...   \n",
            "4  mum turn watch godforsaken movi netflix honest...   \n",
            "\n",
            "                                  Lemmatized_Content  \n",
            "0  black side actually aggravated anything im bla...  \n",
            "1  know one wrote cliched nonsense shallow dialog...  \n",
            "2  time ive bewildered frustrated watching filmth...  \n",
            "3  ever get made old man jonah hill miscast year ...  \n",
            "4  mum turned watch godforsaken movie netflix hon...  \n",
            "Cleaned data saved to cleaned_imdb_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nltk pandas\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "df = pd.read_csv('imdb_reviews.csv')\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(df[['Title', 'Content']].head())\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "df['Cleaned_Content'] = df['Content'].apply(remove_special_characters)\n",
        "print(\"\\nAfter Removing Special Characters:\")\n",
        "print(df[['Content', 'Cleaned_Content']].head())\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "df['Cleaned_Content'] = df['Cleaned_Content'].apply(remove_numbers)\n",
        "print(\"\\nAfter Removing Numbers:\")\n",
        "print(df[['Content', 'Cleaned_Content']].head())\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "df['Cleaned_Content'] = df['Cleaned_Content'].apply(remove_stopwords)\n",
        "print(\"\\nAfter Removing Stopwords:\")\n",
        "print(df[['Content', 'Cleaned_Content']].head())\n",
        "\n",
        "df['Cleaned_Content'] = df['Cleaned_Content'].apply(lambda x: x.lower())\n",
        "print(\"\\nAfter Lowercasing Texts:\")\n",
        "print(df[['Content', 'Cleaned_Content']].head())\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def apply_stemming(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "df['Stemmed_Content'] = df['Cleaned_Content'].apply(apply_stemming)\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(df[['Cleaned_Content', 'Stemmed_Content']].head())\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "df['Lemmatized_Content'] = df['Cleaned_Content'].apply(apply_lemmatization)\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(df[['Stemmed_Content', 'Lemmatized_Content']].head())\n",
        "\n",
        "df.to_csv('cleaned_imdb_reviews.csv', index=False)\n",
        "print(\"Cleaned data saved to cleaned_imdb_reviews.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fa789e-83f0-4c57-b584-9548b9e4c309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parts of Speech Counts:\n",
            "{'Noun': 43319, 'Verb': 19189, 'Adjective': 22073, 'Adverb': 9421}\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      S                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "    __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________     \n",
            "black/JJ side/NN actually/RB aggravated/VBD anything/NN im/JJ black/JJ almost/RB embarrassed/JJ seem/NN incredibly/RB racist/JJ jonah/NN hill/NN character/NN tried/VBD seems/VBZ couldnt/JJ acknowledge/NN didnt/NN care/NN white/JJ problem/NN one/CD point/NN almost/RB wanted/VBD scream/NN screen/NN white/JJ family/NN side/NN family/NN theyre/NN ignorance/NN trying/VBG make/VBP girl/JJ feel/NN welcomed/VBD misguided/JJ way/NN time/NN movie/NN actually/RB became/VBD tad/JJ bit/RB painful/JJ watch/NN know/VBP theyre/NN supposed/VBN moral/JJ movie/NN almost/RB promoted/VBD racism/NN opinion/NN wanted/VBD turn/JJ white/JJ mom/NN interest/NN karen/VBD unfairly/RB actually/RB felt/VBD bad/JJ didnt/NN realize/NN brought/VBD attention/NN horrible/JJ way/NN also/RB think/VBP jonah/NNS hill/VBP character/RB much/RB stronger/JJR lauren/JJ london/NN character/NN amiraalso/JJ side/NN note/NN annoyed/VBD eddie/JJ murphy/NN said/VBD n/JJ word/NN every/DT sentence/NN movie/NN took/VBD issue/NN white/JJ people/NNS ive/JJ ive/JJ always/RB issue/VBP ive/JJ felt/NNS like/IN dont/NN like/IN said/VBD dont/NNS say/VBP\n",
            "\n",
            "\n",
            "Total Named Entity Counts:\n",
            "{'PERSON': 0, 'ORGANIZATION': 0, 'GPE': 0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "df = pd.read_csv('cleaned_imdb_reviews.csv')\n",
        "\n",
        "def pos_analysis(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "    for word, tag in pos_tags:\n",
        "        if tag.startswith('NN'):\n",
        "            counts['Noun'] += 1\n",
        "        elif tag.startswith('VB'):\n",
        "            counts['Verb'] += 1\n",
        "        elif tag.startswith('JJ'):\n",
        "            counts['Adjective'] += 1\n",
        "        elif tag.startswith('RB'):\n",
        "            counts['Adverb'] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "total_counts = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "\n",
        "for content in df['Lemmatized_Content']:\n",
        "    counts = pos_analysis(content)\n",
        "    for key in total_counts:\n",
        "        total_counts[key] += counts[key]\n",
        "\n",
        "print(\"Total Parts of Speech Counts:\")\n",
        "print(total_counts)\n",
        "\n",
        "def constituency_parsing(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sent in sentences:\n",
        "        tokens = nltk.word_tokenize(sent)\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        tree = ne_chunk(tagged)\n",
        "        print(\"\\nConstituency Parsing Tree:\")\n",
        "        tree.pretty_print()\n",
        "\n",
        "example_text = df['Lemmatized_Content'].iloc[0]\n",
        "constituency_parsing(example_text)\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sent in sentences:\n",
        "        tokens = nltk.word_tokenize(sent)\n",
        "        tagged = nltk.pos_tag(tokens)\n",
        "        tree = ne_chunk(tagged)\n",
        "        for subtree in tree.subtrees():\n",
        "            if isinstance(subtree, Tree):\n",
        "                if subtree.label() == 'PERSON':\n",
        "                    person = ' '.join([token for token, pos in subtree.leaves()])\n",
        "                    entities.append(('PERSON', person))\n",
        "                elif subtree.label() == 'ORGANIZATION':\n",
        "                    org = ' '.join([token for token, pos in subtree.leaves()])\n",
        "                    entities.append(('ORGANIZATION', org))\n",
        "                elif subtree.label() == 'GPE':\n",
        "                    gpe = ' '.join([token for token, pos in subtree.leaves()])\n",
        "                    entities.append(('GPE', gpe))\n",
        "\n",
        "    entity_counts = Counter(entities)\n",
        "    return entity_counts\n",
        "\n",
        "total_entities = {'PERSON': 0, 'ORGANIZATION': 0, 'GPE': 0}\n",
        "\n",
        "for content in df['Lemmatized_Content']:\n",
        "    counts = named_entity_recognition(content)\n",
        "    for entity_type, count in counts.items():\n",
        "        total_entities[entity_type[0]] += count\n",
        "\n",
        "print(\"\\nTotal Named Entity Counts:\")\n",
        "print(total_entities)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/file/d/1lg2bM0ClsRuH65G_YR_eda0h6f1w5xOX/view?usp=sharing"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Challenges Faced:-\n",
        "# 1. Data Cleaning Complexity:- Ensuring that the data cleaning process was thorough and effective was challenging. Each step, from removing special characters to lemmatization, required careful consideration to ensure that important information was not lost while also making the text suitable for analysis.\n",
        "\n",
        "# 2.Understanding NLP Libraries:- Transitioning from using spaCy to NLTK required a deeper understanding of the functionalities and limitations of NLTK. Adapting to the differences in approaches to tasks like Named Entity Recognition (NER) and parsing took some time.\n",
        "\n",
        "# 3.Debugging Issues:- Encountering LookupError due to missing NLTK resources was initially frustrating. It highlighted the importance of ensuring that all necessary data is downloaded before running NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "# Enjoyable Aspects:-\n",
        "\n",
        "# 1.Hands-On Learning:- Applying natural language processing techniques to real-world data was a rewarding experience. It was satisfying to see how various cleaning techniques could significantly improve the quality of the text data.\n",
        "\n",
        "# 2.Exploring NLP Techniques:- The assignment provided an opportunity to explore different NLP techniques such as Parts of Speech (POS) tagging, constituency parsing, and Named Entity Recognition. Understanding how these techniques work and their applications in text analysis was particularly fascinating.\n",
        "\n",
        "# 3.Data Analysis:- Analyzing the cleaned data for insights and patterns was engaging. The potential to derive meaningful conclusions from user reviews added a layer of excitement to working with text data.\n",
        "\n",
        "\n",
        "# Opinion on Provided Time:-\n",
        "# The time allocated for this assignment seemed reasonable for someone with a basic understanding of Python and natural language processing concepts. However, for those who may be new to NLP or programming in general, additional time might be beneficial to grasp the intricacies involved in text processing and analysis."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}